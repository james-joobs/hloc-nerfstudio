FROM ghcr.io/nerfstudio-project/nerfstudio:latest

# (ì„ íƒ) í•œêµ­ ë¯¸ëŸ¬: ë² ì´ìŠ¤ ì´ë¯¸ì§€ê°€ Debian/Ubuntu ì–´ëŠ ìª½ì¸ì§€ì— ë”°ë¼ ì¼ë¶€ë§Œ ì ìš©ë  ìˆ˜ ìˆìŒ
RUN sed -i 's|http://deb.debian.org|http://ftp.kaist.ac.kr|g' /etc/apt/sources.list* 2>/dev/null || true && \
    sed -i 's|http://security.debian.org|http://ftp.kaist.ac.kr|g' /etc/apt/sources.list* 2>/dev/null || true && \
    sed -i 's|http://archive.ubuntu.com|http://mirror.kakao.com|g' /etc/apt/sources.list* 2>/dev/null || true

# ë„¤íŠ¸ì›Œí¬/ê¶Œí•œ/ìºì‹œ: pip ìºì‹œ ë„ê³ , PyTorch/ëª¨ë¸ ìºì‹œ ê²½ë¡œ í†µì¼
ENV PIP_NO_CACHE_DIR=1 \
    PIP_ROOT_USER_ACTION=ignore \
    PIP_DEFAULT_TIMEOUT=120 \
    TORCH_HOME=/home/user/.cache/torch \
    HF_HOME=/home/user/.cache/huggingface \
    HLOC_CACHE=/home/user/.cache/hloc \
    PYCOLMAP_CUDA_ARCHITECTURES="native" \
    CMAKE_CUDA_ARCHITECTURES="native"

# SuperGluePretrainedNetwork ê²½ë¡œë¥¼ PYTHONPATHì— ì¶”ê°€
ENV PYTHONPATH="/opt/third_party/SuperGluePretrainedNetwork:/opt/third_party"

# pycolmap C++ backendë¥¼ ìœ„í•œ í•„ìˆ˜ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y --no-install-recommends \
      git vim ca-certificates libgl1 libglib2.0-0 \
      build-essential cmake \
      libboost-program-options-dev libboost-filesystem-dev libboost-graph-dev \
      libboost-system-dev libboost-test-dev \
      libeigen3-dev libflann-dev libfreeimage-dev \
      libmetis-dev libgoogle-glog-dev libgtest-dev \
      libsqlite3-dev libglew-dev qtbase5-dev libqt5opengl5-dev \
      libcgal-dev libceres-dev && \
    update-ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# pip ìµœì‹ í™”
RUN python -m pip install --upgrade pip setuptools wheel

# pycolmap ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ í™˜ê²½ ì„¤ì •
# LD_LIBRARY_PATH ì„¤ì •ìœ¼ë¡œ C++ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ë³´ì¥
ENV LD_LIBRARY_PATH="/usr/local/lib:/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}"


# hloc + lightglue + ìœ í‹¸ ì„¤ì¹˜
# - hlocëŠ” gitì—ì„œ ì„¤ì¹˜ (ë²„ì „ í•€ ì•ˆì •í™” ì›í•˜ë©´ íŠ¹ì • ì»¤ë°‹ìœ¼ë¡œ ê³ ì • ê°€ëŠ¥)
# - pycolmap ì˜ì¡´ì„± ë¬´ì‹œí•˜ê³  ì„¤ì¹˜
RUN python -m pip install --break-system-packages \
      --no-deps "git+https://github.com/cvg/Hierarchical-Localization.git" && \
    python -m pip install --break-system-packages \
      lightglue \
      kornia \
      pydegensac \
      opencv-python-headless \
      faiss-cpu \
      h5py \
      scipy \
      tqdm

# Magicleap SuperPoint êµ¬í˜„ì²´ëŠ” pip íŒ¨í‚¤ì§€ê°€ ì•„ë‹ˆë¯€ë¡œ clone í›„ PYTHONPATHë¡œ ë…¸ì¶œ
RUN mkdir -p /opt/third_party && \
    git clone --depth=1 https://github.com/magicleap/SuperGluePretrainedNetwork.git \
      /opt/third_party/SuperGluePretrainedNetwork

# ê°„ë‹¨í•œ í—¬ìŠ¤ì²´í¬: ëª¨ë“ˆ import í™•ì¸ (ì‹¤íŒ¨í•´ë„ ë¹Œë“œ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•ŠìŒ)
RUN python - <<'PY' || true
import importlib
mods = [
  "hloc",
  "lightglue",
  "kornia",
  "SuperGluePretrainedNetwork.models.superpoint",
]
for m in mods:
    print(m, "OK" if importlib.util.find_spec(m) else "MISSING")
PY

# COLMAP ë°”ì´ë„ˆë¦¬ í™•ì¸ ë° pycolmap fallback ì„¤ì •
RUN which colmap && echo "COLMAP binary found at: $(which colmap)" && colmap -h | head -3 || \
    echo "WARNING: COLMAP binary not found"


# hlocê°€ COLMAP ë°”ì´ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
ENV COLMAP_EXE_PATH="/usr/local/bin/colmap" \
    HLOC_COLMAP_PATH="/usr/local/bin/colmap" \
    HLOC_USE_PYCOLMAP="false"

# ğŸ”§ pycolmap ì„¤ì¹˜ - ì›ë˜ ë²„ì „ (C++ ë°±ì—”ë“œ ë¬¸ì œ ë°©ì§€)
RUN echo "=== Installing pycolmap ===" && \
    # í•„ìˆ˜ ì˜ì¡´ì„± ì„¤ì¹˜
    python -m pip install --break-system-packages numpy pybind11 wheel setuptools && \
    # pycolmap ì›ë˜ ë²„ì „ ì„¤ì¹˜
    python -m pip install --break-system-packages pycolmap --verbose && \
    # ì„¤ì¹˜ í™•ì¸
    python -c "import pycolmap; print(f'âœ“ pycolmap {pycolmap.__version__} installed')" && \
    # C++ backend í…ŒìŠ¤íŠ¸
    python -c "import pycolmap._core; print('âœ“ pycolmap._core available')" || \
    echo "âš  pycolmap._core not available but pycolmap installed"

# ì ‘ê·¼ë²• 2: hloc ë„¤íŠ¸ì›Œí¬ ë‹¤ìš´ë¡œë“œ ë°©ì§€ íŒ¨ì¹˜
RUN python - <<'EOF' || true
import os
import sys

# hloc netvlad.py íŒ¨ì¹˜ - ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê¸°
try:
    import hloc
    hloc_path = hloc.__file__.replace('__init__.py', '')
    
    # netvlad.py íŒŒì¼ íŒ¨ì¹˜
    netvlad_file = os.path.join(hloc_path, 'extractors', 'netvlad.py')
    if os.path.exists(netvlad_file):
        with open(netvlad_file, 'r') as f:
            content = f.read()
        
        if 'torch.hub.download_url_to_file(url, checkpoint_path)' in content and 'DOWNLOAD_SKIP_PATCH' not in content:
            # ì´ë¯¸ if not checkpoint_path.exists() ë¸”ë¡ì´ ìˆëŠ” ê²½ìš° ê·¸ ì•ˆì˜ ë‹¤ìš´ë¡œë“œ ì½”ë“œë¥¼ ìˆ˜ì •
            import re
            
            # ì•ˆì „í•œ ìµœì†Œ íŒ¨ì¹˜: ë‹¨ìˆœíˆ ë‹¤ìš´ë¡œë“œ ì²´í¬ë§Œ ìˆ˜ì •
            import re
            
            # 1. ì²« ë²ˆì§¸ ì‹œë„: if not checkpoint_path.exists() ë¸”ë¡ ë‚´ë¶€ì—ì„œë§Œ ë‹¤ìš´ë¡œë“œ ì½”ë“œ ìˆ˜ì •
            download_pattern = r'(\s+)(torch\.hub\.download_url_to_file\(url, checkpoint_path\))'
            replacement = r'\1# \2  # DOWNLOAD_SKIP_PATCH\n\1print(f"ERROR: Model not found at {checkpoint_path}")\n\1print(f"Model should have been pre-downloaded during build.")\n\1url = self.checkpoint_urls[conf["model_name"]]\n\1print(f"Expected URL: {url}")\n\1raise FileNotFoundError(f"Missing NetVLAD model: {checkpoint_path.name}")'
            
            patched_content = re.sub(download_pattern, replacement, content)
            
            # 2. ë§Œì•½ ìœ„ íŒ¨ì¹˜ê°€ ì‘ë™í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ë‹¨ìˆœ êµì²´
            if patched_content == content:
                patched_content = content.replace(
                    'torch.hub.download_url_to_file(url, checkpoint_path)',
                    '''# torch.hub.download_url_to_file(url, checkpoint_path)  # DOWNLOAD_SKIP_PATCH
                print(f"ERROR: Model not found at {checkpoint_path}")
                raise FileNotFoundError(f"Missing NetVLAD model: {checkpoint_path.name}")'''
                )
            
            # íŒ¨ì¹˜ í‘œì‹œ ì¶”ê°€
            patched_content = '# DOWNLOAD_SKIP_PATCH APPLIED\n' + patched_content
            
            with open(netvlad_file, 'w') as f:
                f.write(patched_content)
            print("âœ“ Applied NetVLAD download skip patch to hloc")
            print(f"  Patched file: {netvlad_file}")
    
    # hloc utilsì— pycolmap ìš°íšŒ íŒ¨ì¹˜ ì¶”ê°€
    patch_file = os.path.join(hloc_path, 'utils', 'parsers.py')
    if os.path.exists(patch_file):
        with open(patch_file, 'r') as f:
            content = f.read()
        
        if 'import pycolmap' in content and 'COLMAP_FALLBACK_PATCH' not in content:
            # pycolmap importë¥¼ try-exceptë¡œ ê°ì‹¸ì„œ ì‹¤íŒ¨ ì‹œ None ì²˜ë¦¬
            patched_content = content.replace(
                'import pycolmap',
                '''# COLMAP_FALLBACK_PATCH
try:
    import pycolmap
except (ImportError, RuntimeError):
    pycolmap = None
    print("WARNING: pycolmap C++ backend unavailable, using COLMAP binary")'''
            )
            with open(patch_file, 'w') as f:
                f.write(patched_content)
            print("âœ“ Applied pycolmap fallback patch to hloc")
    
except Exception as e:
    print(f"Patch attempt failed: {e}")
EOF

# ìµœì¢… ìƒíƒœ í™•ì¸ ë° PYTHONPATH ê²€ì¦
RUN python -c "try: import pycolmap; import pycolmap._core; print('âœ“ pycolmap C++ backend available')\nexcept: print('âš  Using COLMAP binary fallback')" && \
    which colmap && echo "âœ“ COLMAP binary ready" || echo "âŒ No COLMAP found"

# ğŸ”§ ë¡œì»¬ì—ì„œ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ ë³µì‚¬ (ë„¤íŠ¸ì›Œí¬ ì—†ì´ ë¹Œë“œ ê°€ëŠ¥)
# ë¨¼ì € ëª¨ë¸ ë””ë ‰í„°ë¦¬ ìƒì„±
RUN mkdir -p /home/user/.cache/torch/hub/checkpoints && \
    mkdir -p /home/user/.cache/torch/hub/netvlad && \
    mkdir -p /home/user/.cache/hloc

# ë¡œì»¬ì—ì„œ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ íŒŒì¼ë“¤ ë³µì‚¬ - ë‹¨ê³„ë³„ í™•ì¸
# 1. ì „ì²´ models_cache ë””ë ‰í„°ë¦¬ë¥¼ ì„ì‹œ ìœ„ì¹˜ë¡œ ë³µì‚¬
COPY services/nerfstudio/models_cache/ /tmp/model_files/

# 2. ë³µì‚¬ëœ íŒŒì¼ë“¤ í™•ì¸ ë° ìµœì¢… ìœ„ì¹˜ë¡œ ì´ë™
RUN echo "=== Verifying copied model files ===" && \
    echo "Contents of /tmp/model_files/:" && \
    ls -la /tmp/model_files/ && \
    echo "" && \
    echo "=== Moving model files to final locations ===" && \
    # .pth íŒŒì¼ë“¤ì„ checkpointsë¡œ ì´ë™
    for file in /tmp/model_files/*.pth; do \
        if [ -f "$file" ]; then \
            filename=$(basename "$file") && \
            echo "Moving $filename to checkpoints..." && \
            cp "$file" "/home/user/.cache/torch/hub/checkpoints/$filename" && \
            echo "âœ“ Moved $filename" ; \
        fi \
    done && \
    # .mat íŒŒì¼ë“¤ì„ netvladë¡œ ì´ë™
    for file in /tmp/model_files/*.mat; do \
        if [ -f "$file" ]; then \
            filename=$(basename "$file") && \
            echo "Moving $filename to netvlad..." && \
            cp "$file" "/home/user/.cache/torch/hub/netvlad/$filename" && \
            echo "âœ“ Moved $filename" ; \
        fi \
    done && \
    # ì„ì‹œ ë””ë ‰í„°ë¦¬ ì •ë¦¬
    rm -rf /tmp/model_files && \
    echo "=== Model file copying completed ==="

# íŒŒì¼ ê¶Œí•œ ì„¤ì • ë° ìµœì¢… í™•ì¸
RUN chmod -R 755 /home/user/.cache && \
    echo "=== Final verification of copied files ===" && \
    echo "Checkpoints directory contents:" && \
    ls -la /home/user/.cache/torch/hub/checkpoints/ && \
    echo "" && \
    echo "Expected LightGlue models:" && \
    for model in superpoint_lightglue.pth disk_lightglue.pth aliked_lightglue.pth sift_lightglue.pth superpoint_v1.pth; do \
        if [ -f "/home/user/.cache/torch/hub/checkpoints/$model" ]; then \
            size=$(du -h "/home/user/.cache/torch/hub/checkpoints/$model" | cut -f1) && \
            echo "  âœ“ $model ($size)" ; \
        else \
            echo "  âœ— $model NOT FOUND!" ; \
        fi \
    done && \
    echo "" && \
    echo "NetVLAD directory contents:" && \
    ls -la /home/user/.cache/torch/hub/netvlad/

# ğŸ”§ hloc ë° feature extractor ëª¨ë¸ í™•ì¸ ë° ì¶”ê°€ ì„¤ì •
RUN echo "=== Verifying pre-copied models ===" && \
    python - <<'PY'
import torch
import os
from pathlib import Path

# ìºì‹œ ë””ë ‰í„°ë¦¬ í™•ì¸
torch_cache = Path("/home/user/.cache/torch/hub/checkpoints")
netvlad_dir = Path("/home/user/.cache/torch/hub/netvlad")
hloc_cache = Path("/home/user/.cache/hloc")

print("=== Checking pre-copied models ===")

# 1. LightGlue ëª¨ë¸ í™•ì¸
print("\nâœ“ LightGlue models in torch hub cache:")
if torch_cache.exists():
    for f in sorted(torch_cache.glob("*.pth")):
        size_mb = f.stat().st_size / (1024 * 1024)
        print(f"  - {f.name}: {size_mb:.1f} MB")
else:
    print(f"  ERROR: Directory not found: {torch_cache}")

# 2. NetVLAD ëª¨ë¸ í™•ì¸  
print("\nâœ“ NetVLAD models:")
if netvlad_dir.exists():
    for f in sorted(netvlad_dir.glob("*.mat")):
        size_mb = f.stat().st_size / (1024 * 1024)
        print(f"  - {f.name}: {size_mb:.1f} MB")
else:
    print(f"  ERROR: Directory not found: {netvlad_dir}")

# 3. hloc extractor ì„¤ì • í™•ì¸
try:
    import hloc.extractors
    extractors = ["superpoint_aachen", "superpoint_max", "sift"]
    print("\nâœ“ Available hloc extractors:")
    for ext_name in extractors:
        if ext_name in hloc.extractors.confs:
            print(f"  - {ext_name}")
except Exception as e:
    print(f"âš  hloc extractor check failed: {e}")

print("\nâœ… All pre-copied models verified")

PY

# ğŸ”§ LightGlue ë„¤íŠ¸ì›Œí¬ ë‹¤ìš´ë¡œë“œ ë°©ì§€ë¥¼ ìœ„í•œ ê°•ë ¥í•œ íŒ¨ì¹˜
RUN python - <<'EOF'
import os
import sys
import shutil
from pathlib import Path

print("=== Applying LightGlue offline patch ===")

try:
    # 1. ë¨¼ì € LightGlue íŒ¨í‚¤ì§€ì˜ ìœ„ì¹˜ í™•ì¸
    import lightglue
    lightglue_path = Path(lightglue.__file__).parent
    lightglue_file = lightglue_path / 'lightglue.py'
    
    print(f"LightGlue location: {lightglue_path}")
    
    # 2. ê¸°ì¡´ íŒŒì¼ ë°±ì—…
    backup_file = lightglue_path / 'lightglue.py.backup'
    if not backup_file.exists():
        shutil.copy(lightglue_file, backup_file)
        print(f"âœ“ Backup created: {backup_file}")
    
    # 3. íŒŒì¼ ë‚´ìš© ì½ê¸°
    with open(lightglue_file, 'r') as f:
        content = f.read()
    
    # 4. torch.hub.load_state_dict_from_urlì„ ì˜¤í”„ë¼ì¸ ë²„ì „ìœ¼ë¡œ ì™„ì „íˆ êµì²´
    if 'LIGHTGLUE_OFFLINE_PATCH' not in content:
        # íŒ¨ì¹˜ ì½”ë“œ ì‚½ì… (íŒŒì¼ ì‹œì‘ ë¶€ë¶„ì—)
        patch_code = '''# LIGHTGLUE_OFFLINE_PATCH
import torch
from pathlib import Path

def _offline_load_state_dict_from_url(url, *args, **kwargs):
    """Offline version that loads pre-downloaded models"""
    # URLì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
    model_name = url.split('/')[-1]
    
    # torch.hub ê¸°ë³¸ ìºì‹œ ê²½ë¡œ
    cache_dir = Path("/home/user/.cache/torch/hub/checkpoints")
    
    # ì‹¤ì œ íŒŒì¼ëª…ê³¼ ì •í™•íˆ ë§¤ì¹­ (ë¡œì»¬ì— ì €ì¥ëœ íŒŒì¼ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    possible_files = [
        cache_dir / model_name,  # ì›ë³¸ íŒŒì¼ëª… ê·¸ëŒ€ë¡œ
    ]
    
    # LightGlue ëª¨ë¸ ë§¤í•‘ (URLì˜ íŒŒì¼ëª… -> ë¡œì»¬ íŒŒì¼ëª…)
    if 'superpoint_lightglue.pth' in model_name:
        possible_files.insert(0, cache_dir / "superpoint_lightglue.pth")
    elif 'disk_lightglue.pth' in model_name:
        possible_files.insert(0, cache_dir / "disk_lightglue.pth")
    elif 'aliked_lightglue.pth' in model_name:
        possible_files.insert(0, cache_dir / "aliked_lightglue.pth")
    elif 'sift_lightglue.pth' in model_name:
        possible_files.insert(0, cache_dir / "sift_lightglue.pth")
    elif 'superpoint_v1.pth' in model_name:
        possible_files.insert(0, cache_dir / "superpoint_v1.pth")
    
    # íŒŒì¼ ì°¾ê¸° ë° ë¡œë“œ
    for file_path in possible_files:
        if file_path.exists():
            print(f"Loading pre-downloaded model: {file_path}")
            return torch.load(file_path, map_location=kwargs.get('map_location', 'cpu'))
    
    # ë””ë²„ê¹… ì •ë³´
    print(f"ERROR: Could not find pre-downloaded model for URL: {url}")
    print(f"Model name extracted: {model_name}")
    print(f"Searched locations: {[str(p) for p in possible_files]}")
    print(f"Available files in cache:")
    if cache_dir.exists():
        for f in cache_dir.glob('*.pth'):
            print(f"  - {f.name} ({f.stat().st_size / (1024*1024):.1f} MB)")
    raise FileNotFoundError(f"Pre-downloaded model not found for: {model_name}")

# torch.hub.load_state_dict_from_urlì„ ì˜¤í”„ë¼ì¸ ë²„ì „ìœ¼ë¡œ êµì²´
original_torch_hub_load = torch.hub.load_state_dict_from_url
torch.hub.load_state_dict_from_url = _offline_load_state_dict_from_url
'''
        
        # ê¸°ì¡´ import ë¬¸ ë’¤ì— íŒ¨ì¹˜ ì½”ë“œ ì‚½ì…
        import_section_end = content.find('\nclass ')
        if import_section_end > 0:
            patched_content = content[:import_section_end] + '\n\n' + patch_code + '\n' + content[import_section_end:]
        else:
            patched_content = patch_code + '\n\n' + content
        
        # íŒŒì¼ ì €ì¥
        with open(lightglue_file, 'w') as f:
            f.write(patched_content)
        
        print("âœ“ LightGlue offline patch applied successfully")
        print(f"  Patched file: {lightglue_file}")
    else:
        print("âœ“ LightGlue already patched")
    
    # 5. hlocì˜ lightglue matcherë„ íŒ¨ì¹˜ - import ì§ì „ì— torch.hub í•¨ìˆ˜ êµì²´ ë³´ì¥
    try:
        import hloc.matchers.lightglue as hloc_lg
        hloc_lg_file = Path(hloc_lg.__file__)
        
        # hloc matcher íŒŒì¼ ìˆ˜ì • - lightglue import ì „ì— íŒ¨ì¹˜ ì ìš©
        with open(hloc_lg_file, 'r') as f:
            hloc_content = f.read()
        
        if 'HLOC_LIGHTGLUE_OFFLINE_PATCH' not in hloc_content:
            # import lightglue ì „ì— íŒ¨ì¹˜ ì½”ë“œ ì‚½ì…
            patch_insert = '''# HLOC_LIGHTGLUE_OFFLINE_PATCH
import torch
from pathlib import Path

# torch.hub í•¨ìˆ˜ë¥¼ ì˜¤í”„ë¼ì¸ ë²„ì „ìœ¼ë¡œ ë¨¼ì € êµì²´
def _offline_load_state_dict_from_url(url, *args, **kwargs):
    """Offline version that loads pre-downloaded models"""
    model_name = url.split('/')[-1]
    cache_dir = Path("/home/user/.cache/torch/hub/checkpoints")
    
    # ì§ì ‘ íŒŒì¼ëª… ë§¤ì¹­
    if cache_dir.exists():
        target_file = cache_dir / model_name
        if target_file.exists():
            print(f"[HLOC] Loading pre-downloaded model: {target_file}")
            return torch.load(target_file, map_location=kwargs.get('map_location', 'cpu'))
    
    raise FileNotFoundError(f"[HLOC] Pre-downloaded model not found: {model_name}")

torch.hub.load_state_dict_from_url = _offline_load_state_dict_from_url

'''
            # íŒŒì¼ ì•ë¶€ë¶„ì— íŒ¨ì¹˜ ì‚½ì…
            hloc_patched = patch_insert + hloc_content
            
            with open(hloc_lg_file, 'w') as f:
                f.write(hloc_patched)
            
            print("âœ“ hloc LightGlue matcher patched for offline mode")
    except Exception as e:
        print(f"âš  hloc patch skipped: {e}")
    
except Exception as e:
    print(f"ERROR: LightGlue patch failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("âœ… LightGlue offline patch completed")
EOF

# ğŸ”§ pycolmap í˜¸í™˜ì„± íŒ¨ì¹˜ - PosixPath ë¬¸ì œ í•´ê²°
RUN python - <<'EOF'
import os
import sys
from pathlib import Path

print("=== Applying pycolmap compatibility patch ===")

try:
    # hloc reconstruction.py íŒŒì¼ íŒ¨ì¹˜
    import hloc.reconstruction as hloc_recon
    recon_file = Path(hloc_recon.__file__)
    
    print(f"Patching file: {recon_file}")
    
    # íŒŒì¼ ë‚´ìš© ì½ê¸°
    with open(recon_file, 'r') as f:
        content = f.read()
    
    if 'PYCOLMAP_COMPATIBILITY_PATCH' not in content:
        # pycolmap.import_images í˜¸ì¶œ ë¶€ë¶„ì„ ì°¾ì•„ì„œ Path ê°ì²´ë¥¼ strë¡œ ë³€í™˜
        patch_applied = False
        
        # import_images í•¨ìˆ˜ í˜¸ì¶œ íŒ¨í„´ ì°¾ê¸°
        import re
        
        # pycolmap.import_images í˜¸ì¶œ ë¶€ë¶„ íŒ¨ì¹˜
        pattern = r'(\s+)pycolmap\.import_images\(\s*database,\s*image_dir,\s*camera_mode'
        replacement = r'\1# PYCOLMAP_COMPATIBILITY_PATCH: Convert Path objects to strings\n\1pycolmap.import_images(str(database), str(image_dir), camera_mode'
        
        patched_content = re.sub(pattern, replacement, content)
        
        if patched_content != content:
            patch_applied = True
            content = patched_content
            print("âœ“ Applied Path-to-string conversion patch")
        
        # ë‹¤ë¥¸ íŒ¨í„´ë„ í™•ì¸ (kwargs ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
        if 'image_names=' in content and 'options=' in content:
            # kwargs ë²„ì „ë„ íŒ¨ì¹˜
            pattern2 = r'(\s+)pycolmap\.import_images\(\s*([^,]+),\s*([^,]+),\s*([^;]+);\s*kwargs:'
            if 'kwargs:' in content:
                # kwargs íŒ¨í„´ì´ ìˆë‹¤ë©´ ë” ì •êµí•œ íŒ¨ì¹˜ í•„ìš”
                lines = content.split('\n')
                for i, line in enumerate(lines):
                    if 'pycolmap.import_images(' in line and 'kwargs:' in content[content.find(line):content.find(line)+200]:
                        # í•´ë‹¹ ë¼ì¸ê³¼ ê·¸ ë‹¤ìŒ ëª‡ ë¼ì¸ì„ ì°¾ì•„ì„œ ìˆ˜ì •
                        if 'database' in line and 'image_dir' in line:
                            lines[i] = line.replace('database,', 'str(database),').replace('image_dir,', 'str(image_dir),')
                            patch_applied = True
                            print("âœ“ Applied kwargs version patch")
                            break
                content = '\n'.join(lines)
        
        # ë” ê°„ë‹¨í•œ ì ‘ê·¼: import_images í˜¸ì¶œì„ ê°ì‹¸ëŠ” wrapper í•¨ìˆ˜ ìƒì„±
        if not patch_applied:
            # ê¸°ì¡´ import pycolmap ë¼ì¸ì„ ì°¾ì•„ì„œ ê·¸ ë‹¤ìŒì— íŒ¨ì¹˜ ì¶”ê°€
            import_line_index = -1
            lines = content.split('\n')
            
            for i, line in enumerate(lines):
                if 'import pycolmap' in line and not line.strip().startswith('#'):
                    import_line_index = i
                    break
            
            if import_line_index >= 0:
# import pycolmap ë¼ì¸ ë‹¤ìŒì— íŒ¨ì¹˜ ì½”ë“œ ì‚½ì…
                wrapper_patch_lines = [
                    '',
                    '# PYCOLMAP_COMPATIBILITY_PATCH',
                    '# Backup original function FIRST',
                    '_original_import_images = pycolmap.import_images',
                    '',
                    'def _patched_import_images(database_path, image_path, camera_mode, image_list=None, options=None, **kwargs):',
                    '    """Wrapper for pycolmap.import_images that converts Path objects to strings"""',
                    '    # Convert Path objects to strings', 
                    '    database_str = str(database_path) if hasattr(database_path, "__fspath__") else database_path',
                    '    image_str = str(image_path) if hasattr(image_path, "__fspath__") else image_path',
                    '    ',
                    '    # Handle kwargs mapping for API compatibility',
                    '    if "image_names" in kwargs:',
                    '        image_list = kwargs.pop("image_names", [])',
                    '    if "options" in kwargs:',
                    '        options = kwargs.pop("options", None)',
                    '    ',
                    '    # Create default options if not provided',
                    '    if options is None:',
                    '        import pycolmap',
                    '        options = pycolmap.ImageReaderOptions()',
                    '    ',
                    '    # Ensure image_list is provided (empty list if None)',
                    '    if image_list is None:',
                    '        image_list = []',
                    '    ',
                    '    # Call with proper positional arguments',
                    '    return _original_import_images(database_str, image_str, camera_mode, image_list, options)',
                    '',
                    '# Replace with patched version',
                    'pycolmap.import_images = _patched_import_images',
                    ''
                ]
                
                # import ë¼ì¸ ë‹¤ìŒì— íŒ¨ì¹˜ ì½”ë“œ ì‚½ì…
                lines[import_line_index+1:import_line_index+1] = wrapper_patch_lines
                content = '\n'.join(lines)
                patch_applied = True
                print("âœ“ Applied wrapper function patch after import statement")
            else:
                print("âš  Could not find 'import pycolmap' line for patching")
        
        if patch_applied:
            # íŒ¨ì¹˜ëœ ë‚´ìš©ì„ íŒŒì¼ì— ì €ì¥
            with open(recon_file, 'w') as f:
                f.write(content)
            print(f"âœ“ pycolmap compatibility patch applied to {recon_file}")
        else:
            print("âš  No suitable pattern found for patching")
    else:
        print("âœ“ pycolmap compatibility patch already applied")

except Exception as e:
    print(f"ERROR: pycolmap compatibility patch failed: {e}")
    import traceback
    traceback.print_exc()

print("âœ… pycolmap compatibility patch completed")
EOF

# SuperGluePretrainedNetwork ë° ì „ì²´ ëª¨ë“ˆ ê²€ì¦
RUN echo "=== Final module verification ===" && \
    ls -al /opt/third_party | grep SuperGluePretrainedNetwork && \
    export PYTHONPATH=/opt/third_party:$PYTHONPATH && \
    python - <<'PY'
import importlib.util as I, sys
print("PYTHON:", sys.executable)
print("PYTHONPATH:", sys.path[:3])
print("hloc:", bool(I.find_spec("hloc")))
print("lightglue:", bool(I.find_spec("lightglue")))
print("SuperGluePretrainedNetwork:", bool(I.find_spec("SuperGluePretrainedNetwork")))
try:
    from SuperGluePretrainedNetwork.models import superpoint
    print("âœ“ SuperGluePretrainedNetwork.models.superpoint import successful")
except Exception as e:
    print("âš  SuperGluePretrainedNetwork import failed:", e)

# ns-process-data í˜¸í™˜ì„± ìµœì¢… í™•ì¸
try:
    import nerfstudio
    print("âœ“ nerfstudio available")
    print("âœ“ All dependencies ready for ns-process-data")
except Exception as e:
    print("âš  nerfstudio issue:", e)

# ìµœì¢… ê²€ì¦: ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸
print("\n=== Final Model Location Verification ===")
from pathlib import Path
cache_dir = Path("/home/user/.cache/torch/hub/checkpoints")
required_models = [
    "superpoint_lightglue.pth",
    "disk_lightglue.pth", 
    "aliked_lightglue.pth",
    "sift_lightglue.pth",
    "superpoint_v1.pth"
]

if cache_dir.exists():
    print(f"Torch hub cache: {cache_dir}")
    models = list(cache_dir.glob("*.pth"))
    print(f"Found {len(models)} model files:")
    for m in sorted(models):
        size_mb = m.stat().st_size / (1024 * 1024)
        status = "âœ“" if m.name in required_models else "?"
        print(f"  {status} {m.name}: {size_mb:.1f} MB")
    
    # í•„ìˆ˜ ëª¨ë¸ í™•ì¸
    missing = []
    for req_model in required_models:
        if not (cache_dir / req_model).exists():
            missing.append(req_model)
    
    if missing:
        print(f"\nâš  WARNING: Missing required models: {missing}")
    else:
        print("\nâœ“ All required models are present")
else:
    print(f"ERROR: Cache directory not found: {cache_dir}")

# LightGlue íŒ¨ì¹˜ í™•ì¸
try:
    import lightglue
    lg_file = Path(lightglue.__file__).parent / 'lightglue.py'
    with open(lg_file, 'r') as f:
        content = f.read()
    if 'LIGHTGLUE_OFFLINE_PATCH' in content:
        print("âœ“ LightGlue offline patch is active")
    else:
        print("âš  LightGlue offline patch NOT found")
        
    # hloc íŒ¨ì¹˜ í™•ì¸
    import hloc.matchers.lightglue as hloc_lg
    hloc_file = Path(hloc_lg.__file__)
    with open(hloc_file, 'r') as f:
        hloc_content = f.read()
    if 'HLOC_LIGHTGLUE_OFFLINE_PATCH' in hloc_content:
        print("âœ“ HLOC LightGlue matcher offline patch is active")
    else:
        print("âš  HLOC patch NOT found")
        
except Exception as e:
    print(f"âš  Could not verify patches: {e}")
PY

# ğŸ”§ DataLoader shared memory ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
ENV OMP_NUM_THREADS=1
ENV PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512"
# DataLoader workers ì œí•œ (shared memory ë¶€ì¡± ë°©ì§€)
ENV TORCH_NUM_WORKERS=0

WORKDIR /workspace